{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"no pooling Transfer2People.ipynb","provenance":[{"file_id":"1k06rrz-6hsX_SEje3M6l8e9IdfmSiGX1","timestamp":1578483035129}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VyaAE5Bn3gD3","colab_type":"code","outputId":"07ea98ca-2d0e-4fb3-9fa5-e37aa96c3115","executionInfo":{"status":"ok","timestamp":1578491788989,"user_tz":-480,"elapsed":6562,"user":{"displayName":"史孟玄","photoUrl":"","userId":"05623711265995213758"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["# https://drive.google.com/uc?id=1eJnK7Vhj7ZwkVuqFZ9jl3szWdXbFbtKG\n","import gdown\n","\n","!gdown https://drive.google.com/uc?id=1eJnK7Vhj7ZwkVuqFZ9jl3szWdXbFbtKG #https://drive.google.com/uc?id=1lLDZwQ0JiUM9FxTPGUGNQJhzBEkgm7x4\n","!unzip -nq *.zip\n","\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1eJnK7Vhj7ZwkVuqFZ9jl3szWdXbFbtKG\n","To: /content/CamouflageData.zip\n","123MB [00:00, 142MB/s] \n","CamouflageData/Camouﬂage pattern statement.txt:  mismatching \"local\" filename (CamouflageData/CamouямВage pattern statement.txt),\n","         continuing with \"central\" filename version\n","CamouflageData\tCamouflageData.zip  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nG1a3tMiD9J2","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset\n","from torchvision import utils\n","import torchvision\n","from torchvision import models\n","from torchvision.models.vgg import VGG\n","import random\n","\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import time\n","import sys\n","import os\n","from os import path\n","\n","from PIL import Image\n","import pandas as pd\n","from torchvision.models.vgg import VGG\n","\n","from pathlib import Path"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0VlyzAaD-s7","colab_type":"code","outputId":"45c2ce0d-2d49-4f78-c4ac-d080f3e154c5","executionInfo":{"status":"ok","timestamp":1578491789657,"user_tz":-480,"elapsed":7206,"user":{"displayName":"史孟玄","photoUrl":"","userId":"05623711265995213758"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["root_dir = '/content/CamouflageData/'\n","train_file = root_dir + 'train.csv'\n","\n","print(\"training csv exists:{}\".format(path.exists(train_file)))\n","\n","# the folder to save results for comparison\n","folder_to_save_validation_result = root_dir + '/result_comparision/' \n","\n","if os.path.isdir(folder_to_save_validation_result) == False:\n","    os.mkdir(folder_to_save_validation_result)\n","\n","# the number of segmentation classes\n","num_class = 2 # 32 for original CamVid\n","means     = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\n","\n","h, w      = 256, 256\n","train_h = 256\n","train_w = 256\n","val_h = 256\n","val_w = 256\n","\n","## parameters for Solver-Adam in this example\n","batch_size = 6 #\n","epochs     = 20 # don't try to improve the performance by simply increasing the training epochs or iterations\n","lr         = 1e-4    # achieved besty results \n","step_size  = 100 # Won't work when epochs <=100\n","gamma      = 0.5 # \n","#\n","\n","## index for validation images\n","global_index = 0\n","\n","# pixel accuracy and mIOU list \n","pixel_acc_list = []\n","mIOU_list = []\n","f_measure_list = []\n","mae_list = []\n","\n","use_gpu = torch.cuda.is_available()\n","num_gpu = list(range(torch.cuda.device_count()))\n","\n","class CamVidDataset(Dataset):\n","\n","    def __init__(self, csv_file, phase, n_class=num_class, crop=True, flip_rate=0.5):\n","        self.data      = pd.read_csv(csv_file)\n","        self.means     = means\n","        self.n_class   = n_class\n","        self.flip_rate = flip_rate       \n","\n","        self.resize_h = h\n","        self.resize_w = w        \n","        \n","        if phase == 'train':\n","            self.new_h = train_h\n","            self.new_w = train_w\n","            self.crop = crop\n","        elif phase == 'val':\n","            self.flip_rate = 0.\n","            self.crop = False # False\n","            self.new_h = val_h\n","            self.new_w = val_w\n","\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_name   = self.data.iloc[idx, 0]                \n","        img_name = root_dir  + img_name                        \n","        img = Image.open(img_name).convert('RGB')  \n","        # img = img.resize((480, 360))\n","        img = img.resize((256, 256))\n","        label_name = self.data.iloc[idx, 1]        \n","        label_name = root_dir  + label_name                       \n","        label_image = Image.open(label_name).convert('L')\n","        # label_image = label_image.resize((480, 360))\n","        label_image = label_image.resize((256, 256))\n","        label = np.asarray(label_image)\n","\n","\n","\n","        # In training mode, the crop strategy is random-shift crop.\n","        # In validation model, it is center crop.\n","        # if self.crop:            \n","        #     w, h = img.size\n","        #     A_x_offset = np.int32(np.random.randint(0, w - self.new_w + 1, 1))[0]\n","        #     A_y_offset = np.int32(np.random.randint(0, h - self.new_h + 1, 1))[0]\n","\n","        #     img = img.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","        #     label_image = label_image.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","        # else:            \n","        #     w, h = img.size\n","        #     A_x_offset = int((w - self.new_w)/2)\n","        #     A_y_offset = int((h - self.new_h)/2)\n","            \n","        #     img = img.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","        #     label_image = label_image.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","\n","        #     label_image_h, label_image_w = label_image.size\n","\n","        # we could try to revise the values in label for reducing the number of segmentation classes\n","        label = np.array(label_image)              \n","        label = label/max(label.max(),1)\n","        if random.random() < self.flip_rate:\n","            img   = np.fliplr(img)\n","            label = np.fliplr(label)\n","        \n","        # reduce mean in terms of BGR\n","        img = np.transpose(img, (2, 0, 1)) / 255.\n","        img[0] -= self.means[0]\n","        img[1] -= self.means[1]\n","        img[2] -= self.means[2]\n","\n","        # convert to tensor\n","        img = torch.from_numpy(img.copy()).float()\n","        label = torch.from_numpy(label.copy()).long()\n","\n","        # create one-hot encoding\n","        lst = [x for x in label.size()]\n","        if len(lst)>2:\n","          print(label_name)\n","          print(lst)\n","        h, w = label.size()\n","        target = torch.zeros(self.n_class, h, w)\n","        for c in range(self.n_class):\n","            target[c][label == c] = 1\n","\n","        sample = {'X': img, 'Y': target, 'l': label, 'N': label_name}\n","\n","        return sample\n","\n","\n","train_data = CamVidDataset(csv_file=train_file, phase='train')\n","val_data = CamVidDataset(csv_file=train_file, phase='val')\n","# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n","pivot = len(train_data) * 3 // 5\n","train_data = Subset(train_data, range(0, pivot))\n","val_data = Subset(val_data, range(pivot, len(val_data)))\n","print(len(train_data), len(val_data))\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n","val_loader = DataLoader(val_data, batch_size=1, num_workers=8)\n","\n","# x = next(iter(train_loader))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["training csv exists:True\n","599 400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dKraXnLeD-lV","colab_type":"code","outputId":"54f5b556-6021-44ca-bac4-385e2d66f05c","executionInfo":{"status":"ok","timestamp":1578493301723,"user_tz":-480,"elapsed":5269,"user":{"displayName":"史孟玄","photoUrl":"","userId":"05623711265995213758"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cfg = {\n","    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","ranges = {\n","    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n","    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n","    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n","    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n","}\n","\n","def make_layers(cfg, batch_norm=False):\n","    layers = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","\n","class VGGNet(VGG):\n","    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n","        super().__init__(make_layers(cfg[model]))\n","        self.ranges = ranges[model]\n","\n","        if pretrained:            \n","            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n","\n","        if not requires_grad:\n","            for param in super().parameters():\n","                param.requires_grad = False\n","\n","        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n","            del self.classifier\n","\n","        if show_params:\n","            for name, param in self.named_parameters():\n","                print(name, param.size())\n","\n","    def forward(self, x):\n","        output = {}\n","        # get the output of each maxpooling layer (5 maxpool in VGG net)\n","        for idx in range(len(self.ranges)):\n","            for layer in range(self.ranges[idx][0], self.ranges[idx][1]-1):\n","                x = self.features[layer](x)\n","            output[\"x%d\"%(idx+1)] = x\n","            x = self.features[self.ranges[idx][1]-1](x)\n","        return output\n","\n","        # for idx in range(len(self.ranges)):\n","        #     for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n","        #         x = self.features[layer](x)\n","        #     output[\"x%d\"%(idx+1)] = x\n","        # return output\n","\n","class FCN8s(nn.Module):\n","    #Ref: https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1\n","    #The layer description is accordance with the above fiture instead of the original paper. Alex 2019/12/03 \n","    def __init__(self, pretrained_net, n_class):\n","        super().__init__()\n","        self.n_class = n_class\n","        self.pretrained_net = pretrained_net\n","        self.relu    = nn.ReLU(inplace=True)\n","        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn1     = nn.BatchNorm2d(512)\n","        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn2     = nn.BatchNorm2d(256)\n","        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn3     = nn.BatchNorm2d(128)\n","        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn4     = nn.BatchNorm2d(64)\n","        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn5     = nn.BatchNorm2d(32)\n","        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n","\n","\n","\n","    def forward(self, x):\n","        output = self.pretrained_net(x)\n","        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n","        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n","        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n","        # print('init', x5.shape)\n","        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n","        # print('1', score.shape)       \n","        score = self.bn1(score+x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)                      \n","        score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)\n","        # print('2', score.shape)       \n","        score = self.bn2(score+x3)                      # element-wise add, size=(N, 256, x.H/8, x.W/8)         \n","        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n","        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n","        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n","        # print('L', score.shape)\n","        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n","        m = nn.AdaptiveAvgPool2d((256,256))\n","        score = m(score)\n","        return score  # size=(N, n_class, x.H/1, x.W/1)\n","\n","# load pretrained weights and define FCN8s\n","vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n","fcn_model = FCN8s(pretrained_net=vgg_model, n_class=num_class)\n","\n","ts = time.time()\n","vgg_model = vgg_model.cuda()\n","fcn_model = fcn_model.cuda()\n","fcn_model = nn.DataParallel(fcn_model, device_ids=num_gpu)\n","print(\"Finish cuda loading, time elapsed {}\".format(time.time() - ts))\n","\n","# criterion=new_loss\n","# criterion = nn.BCEWithLogitsLoss()\n","# criterion = FocalLoss()\n","# criterion = DiceLoss()\n","optimizer = optim.Adam(fcn_model.parameters(), lr=lr)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Finish cuda loading, time elapsed 0.01999378204345703\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LU9dUPEMXTZN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UADtIeFHD-hk","colab_type":"code","colab":{}},"source":["def train():\n","    for epoch in range(epochs):\n","        scheduler.step()\n","\n","        ts = time.time()\n","        for iter, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","\n","            if use_gpu:\n","                inputs = Variable(batch['X'].cuda())\n","                labels = Variable(batch['Y'].cuda())\n","            else:\n","                inputs, labels = Variable(batch['X']), Variable(batch['Y'])\n","            outputs = fcn_model(inputs)\n","            # # !!!!!!!\n","            # print(outputs)\n","            \n","            # print(\"!!!!!!\")\n","            # print(labels)\n","            # # !!!!!!!\n","            # weights=[1/((labels==1).numel()),1/((labels==0).numel())]\n","            # pos_weight=torch.tensor((labels==0).numel()/(labels==1).numel()).cuda()*1.5\n","            criterion=nn.BCEWithLogitsLoss()\n","            # loss=criterion.forward(input=m(outputs),target=labels.type(torch.LongTensor).cuda())\n","\n","            labels/=max(labels.max(),1)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if iter % 10 == 0:\n","                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss.data.item()))\n","        \n","        print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n","        \n","\n","        val(epoch)\n","        fcn_model.train()\n","        \n","    highest_pixel_acc = max(pixel_acc_list)\n","    highest_mIOU = max(mIOU_list)\n","    highest_f_measure = max(f_measure_list)\n","    lowest_mae = min(mae_list)        \n","    \n","    highest_pixel_acc_epoch = pixel_acc_list.index(highest_pixel_acc)\n","    highest_mIOU_epoch = mIOU_list.index(highest_mIOU)\n","    highest_f_measure_epoch = f_measure_list.index(highest_f_measure)\n","    lowest_mae_epoch = mae_list.index(lowest_mae)\n","    \n","    print(\"The highest mIOU is {} and is achieved at epoch-{}\".format(highest_mIOU, highest_mIOU_epoch))\n","    print(\"The lowest MAE is {} and is achieved at epoch-{}\".format(lowest_mae, lowest_mae_epoch))\n","    print(\"The highest f-measure is {} and is achieved at epoch-{}\".format(highest_f_measure, highest_f_measure_epoch))\n","\n","\n","def save_result_comparison(input_np, output_np, gt_path):\n","    print(gt_path)\n","    means     = np.array([103.939, 116.779, 123.68]) / 255.\n","    \n","    global global_index\n","    \n","    original_im_RGB = np.zeros((256,256,3))    \n","    original_im_RGB[:,:,0] = input_np[0,0,:,:]    \n","    original_im_RGB[:,:,1] = input_np[0,1,:,:]\n","    original_im_RGB[:,:,2] = input_np[0,2,:,:]\n","        \n","    original_im_RGB[:,:,0] = original_im_RGB[:,:,0] + means[0]\n","    original_im_RGB[:,:,1] = original_im_RGB[:,:,1] + means[1]\n","    original_im_RGB[:,:,2] = original_im_RGB[:,:,2] + means[2]\n","        \n","    original_im_RGB[:,:,0] = original_im_RGB[:,:,0]*255.0\n","    original_im_RGB[:,:,1] = original_im_RGB[:,:,1]*255.0\n","    original_im_RGB[:,:,2] = original_im_RGB[:,:,2]*255.0\n","    \n","    im_seg_RGB = np.zeros((256,256,3))\n","\n","    # the following version is designed for 11-class version and could still work if the number of classes is fewer.\n","    for i in range(256):\n","        for j in range(256):\n","            if output_np[i,j] == 0:\n","                im_seg_RGB[i,j,:] = [0, 0, 0]\n","            elif output_np[i,j] == 1:  \n","                im_seg_RGB[i,j,:] = [255, 255, 255]\n","            elif output_np[i,j] == 2:  \n","                im_seg_RGB[i,j,:] = [192, 192, 128]    \n","            elif output_np[i,j] == 3:  \n","                im_seg_RGB[i,j,:] = [128, 64, 128]    \n","            elif output_np[i,j] == 4:  \n","                im_seg_RGB[i,j,:] = [0, 0, 192]    \n","            elif output_np[i,j] == 5:  \n","                im_seg_RGB[i,j,:] = [128, 128, 0]    \n","            elif output_np[i,j] == 6:  \n","                im_seg_RGB[i,j,:] = [192, 128, 128]    \n","            elif output_np[i,j] == 7:  \n","                im_seg_RGB[i,j,:] = [64, 64, 128]    \n","            elif output_np[i,j] == 8:  \n","                im_seg_RGB[i,j,:] = [64, 0, 128]    \n","            elif output_np[i,j] == 9:  \n","                im_seg_RGB[i,j,:] = [64, 64, 0]    \n","            elif output_np[i,j] == 10:  \n","                im_seg_RGB[i,j,:] = [0, 128, 192]    \n","                    \n","    # horizontally stack original image and its corresponding segmentation results \n","    gt_image = Image.open(gt_path).convert('RGB')\n","    gt_image = gt_image.resize((256, 256))\n","    slicing_vertical = np.ones((256, 2, 3)) * 255.0\n","    hstack_image = np.hstack((original_im_RGB, slicing_vertical, im_seg_RGB, slicing_vertical, gt_image))             \n","    return hstack_image\n","    \n","def save_image(image_stack):\n","    global global_index\n","    stack = []\n","    slicing_horizontal = np.ones((2, 772, 3)) * 255.0\n","    for i in image_stack:\n","        stack.append(i)\n","        stack.append(slicing_horizontal)\n","    vstack_image = np.vstack(stack)\n","    new_im = Image.fromarray(np.uint8(vstack_image))\n","    \n","    file_name = folder_to_save_validation_result + str(global_index) + '.jpg'\n","        \n","    global_index = global_index + 1\n","        \n","    new_im.save(file_name)  \n","\n","# train()     "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cl0WVeoTD-be","colab_type":"code","colab":{}},"source":["def val(epoch):\n","    fcn_model.eval()\n","    total_ious = []\n","    pixel_accs = []\n","    f_measures = []\n","    maes = []\n","    numberOfImage = 4\n","    \n","    for iter, batch in enumerate(val_loader): ## batch is 1 in this case\n","        if use_gpu:\n","            inputs = Variable(batch['X'].cuda())\n","        else:\n","            inputs = Variable(batch['X'])        \n","\n","        output = fcn_model(inputs)                                \n","        \n","        # only save the 1st image for comparison\n","\n","        if iter <= numberOfImage:\n","            print('---------iter={}'.format(iter))\n","            if iter == 0:\n","                vstack_image = [] \n","            # generate images\n","            images = output.data.max(1)[1].cpu().numpy()[:,:,:]\n","            image = images[0,:,:]        \n","            image = save_result_comparison(batch['X'], image, batch['N'][0])\n","            vstack_image.append(image)\n","            print(batch['N'])\n","            if iter == numberOfImage:\n","                save_image(vstack_image)\n","\n","                            \n","        output = output.data.cpu().numpy()\n","\n","        N, _, h, w = output.shape                \n","        pred = output.transpose(0, 2, 3, 1).reshape(-1, num_class).argmax(axis=1).reshape(N, h, w)        \n","        target = batch['l'].cpu().numpy().reshape(N, h, w)\n","\n","        for p, t in zip(pred, target):\n","            total_ious.append(iou(p, t))\n","            pixel_accs.append(pixel_acc(p, t))\n","            f_measures.append(F_Measure(p, t))\n","            maes.append(MAE(p, t))\n","\n","    # Calculate average IoU\n","    total_ious = np.array(total_ious).T  # n_class * val_len\n","    ious = np.nanmean(total_ious, axis=1)\n","    pixel_accs = np.array(pixel_accs).mean()\n","    f_measures = np.nanmean(np.array(f_measures))\n","    maes = np.array(maes).mean()\n","    print(\"epoch{}, MAE: {}, meanIoU: {}, f_measure: {}, IoUs: {}\".format(epoch, maes, np.nanmean(ious), f_measures, ious))\n","    \n","    global pixel_acc_list\n","    global mIOU_list\n","    global f_measure_list\n","    global mae_list\n","    \n","    pixel_acc_list.append(pixel_accs)\n","    mIOU_list.append(np.nanmean(ious))\n","    f_measure_list.append(f_measures)\n","    mae_list.append(maes)\n","\n","\n","# borrow functions and modify it from https://github.com/Kaixhin/FCN-semantic-segmentation/blob/master/main.py\n","# Calculates class intersections over unions\n","def iou(pred, target):\n","    ious = []\n","    target=target/max(target.max(),1)\n","    for cls in range(num_class):\n","        pred_inds = pred == cls\n","        target_inds = target == cls\n","        intersection = pred_inds[target_inds].sum()\n","        union = pred_inds.sum() + target_inds.sum() - intersection\n","        # if(cls==1):\n","        #     print(pred_inds.sum())\n","        #     print(target_inds.sum())\n","        #     print(intersection)\n","        if union == 0:\n","            ious.append(float('nan'))  # if there is no ground truth, do not include in evaluation\n","        else:\n","            ious.append(float(intersection) / max(union, 1))\n","        # print(\"cls\", cls, pred_inds.sum(), target_inds.sum(), intersection, float(intersection) / max(union, 1))\\\n","    \n","    return ious\n","\n","\n","def pixel_acc(pred, target):\n","    correct = (pred == target).sum()\n","    total   = (target == target).sum()\n","    return correct / total\n","\n","def F_Measure(pred, target):\n","    beta_sqr = 0.3\n","    target=target/max(target.max(),1)\n","    cls = 1\n","    pred_inds = pred == cls\n","    target_inds = target == cls\n","    TP = pred_inds[target_inds].sum()\n","    FP = pred_inds.sum() - TP\n","    FN = target_inds.sum() - TP\n","    P = TP / (TP + FP)\n","    R = TP / (TP + FN)\n","    denominator = (beta_sqr*P + R)\n","    # print(P, R)\n","    if denominator == 0:\n","        return float('nan') # if there is no ground truth, do not include in evaluation\n","    else:\n","        f_measure = (beta_sqr + 1) * P * R / denominator\n","        return f_measure\n","\n","def MAE(pred, target):\n","    target=target/max(target.max(),1)\n","    pred = torch.from_numpy(pred).float()\n","    target = torch.from_numpy(target).float()\n","    # print(type(target[0][0]))\n","    loss = nn.L1Loss()\n","    mae= loss(pred, target)\n","    return mae\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujsJzHISD-Ru","colab_type":"code","outputId":"b3b22e63-9fa7-478a-cf76-4ecf4e7f335a","executionInfo":{"status":"ok","timestamp":1578495349015,"user_tz":-480,"elapsed":137789,"user":{"displayName":"史孟玄","photoUrl":"","userId":"05623711265995213758"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## perform training and validation\n","global_index = 0\n","pixel_acc_list = []\n","mIOU_list = []\n","f_measure_list = []\n","mae_list = []\n","# val(0)  # show the accuracy before training\n","train()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["epoch0, iter0, loss: 0.6648925542831421\n","epoch0, iter10, loss: 0.6447578072547913\n","epoch0, iter20, loss: 0.6320264935493469\n","epoch0, iter30, loss: 0.6217685341835022\n","epoch0, iter40, loss: 0.6444047093391418\n","epoch0, iter50, loss: 0.6086833477020264\n","epoch0, iter60, loss: 0.631170392036438\n","epoch0, iter70, loss: 0.5736210346221924\n","epoch0, iter80, loss: 0.6077731847763062\n","epoch0, iter90, loss: 0.6506041884422302\n","Finish epoch 0, time elapsed 76.99594831466675\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch0, MAE: 0.04008220508694649, meanIoU: 0.6100932640692205, f_measure: 0.3140861446118368, IoUs: [0.95886117 0.26132535]\n","epoch1, iter0, loss: 0.5707379579544067\n","epoch1, iter10, loss: 0.6038388609886169\n","epoch1, iter20, loss: 0.6057391166687012\n","epoch1, iter30, loss: 0.5995075702667236\n","epoch1, iter40, loss: 0.5516092777252197\n","epoch1, iter50, loss: 0.5908652544021606\n","epoch1, iter60, loss: 0.5381150245666504\n","epoch1, iter70, loss: 0.5013375282287598\n","epoch1, iter80, loss: 0.5816094279289246\n","epoch1, iter90, loss: 0.5422152280807495\n","Finish epoch 1, time elapsed 76.17380690574646\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch1, MAE: 0.013266143389046192, meanIoU: 0.705480503925072, f_measure: 0.5359617692044925, IoUs: [0.98628371 0.4246773 ]\n","epoch2, iter0, loss: 0.5655533075332642\n","epoch2, iter10, loss: 0.5334638357162476\n","epoch2, iter20, loss: 0.555878758430481\n","epoch2, iter30, loss: 0.5542401075363159\n","epoch2, iter40, loss: 0.5762403607368469\n","epoch2, iter50, loss: 0.5091482996940613\n","epoch2, iter60, loss: 0.5307217240333557\n","epoch2, iter70, loss: 0.5122389793395996\n","epoch2, iter80, loss: 0.5246049165725708\n","epoch2, iter90, loss: 0.5560356974601746\n","Finish epoch 2, time elapsed 76.41064977645874\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch2, MAE: 0.013071442022919655, meanIoU: 0.7335360289579848, f_measure: 0.5927810493026903, IoUs: [0.98640553 0.48066652]\n","epoch3, iter0, loss: 0.5381687879562378\n","epoch3, iter10, loss: 0.4927310347557068\n","epoch3, iter20, loss: 0.5226669311523438\n","epoch3, iter30, loss: 0.5617300271987915\n","epoch3, iter40, loss: 0.5276216268539429\n","epoch3, iter50, loss: 0.5474324226379395\n","epoch3, iter60, loss: 0.4891568422317505\n","epoch3, iter70, loss: 0.5254064798355103\n","epoch3, iter80, loss: 0.42637309432029724\n","epoch3, iter90, loss: 0.489142507314682\n","Finish epoch 3, time elapsed 76.23585844039917\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch3, MAE: 0.011352424509823322, meanIoU: 0.7439063771410278, f_measure: 0.5947258442304856, IoUs: [0.98827572 0.49953704]\n","epoch4, iter0, loss: 0.4219242036342621\n","epoch4, iter10, loss: 0.5016278028488159\n","epoch4, iter20, loss: 0.5060817003250122\n","epoch4, iter30, loss: 0.5076259970664978\n","epoch4, iter40, loss: 0.5052586197853088\n","epoch4, iter50, loss: 0.4994184970855713\n","epoch4, iter60, loss: 0.461509644985199\n","epoch4, iter70, loss: 0.4796885848045349\n","epoch4, iter80, loss: 0.44437310099601746\n","epoch4, iter90, loss: 0.40462514758110046\n","Finish epoch 4, time elapsed 76.08272933959961\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch4, MAE: 0.007936439476907253, meanIoU: 0.7704462946670225, f_measure: 0.6810988327110301, IoUs: [0.99177067 0.54912192]\n","epoch5, iter0, loss: 0.4785204827785492\n","epoch5, iter10, loss: 0.4647655189037323\n","epoch5, iter20, loss: 0.4452711343765259\n","epoch5, iter30, loss: 0.5691013336181641\n","epoch5, iter40, loss: 0.3931218385696411\n","epoch5, iter50, loss: 0.472398579120636\n","epoch5, iter60, loss: 0.45678210258483887\n","epoch5, iter70, loss: 0.44942256808280945\n","epoch5, iter80, loss: 0.37342745065689087\n","epoch5, iter90, loss: 0.44248342514038086\n","Finish epoch 5, time elapsed 76.1416232585907\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch5, MAE: 0.010468444786965847, meanIoU: 0.7402851284818481, f_measure: 0.6139652773411969, IoUs: [0.98918985 0.49138041]\n","epoch6, iter0, loss: 0.3666827082633972\n","epoch6, iter10, loss: 0.4488804042339325\n","epoch6, iter20, loss: 0.43252795934677124\n","epoch6, iter30, loss: 0.4450992941856384\n","epoch6, iter40, loss: 0.39009928703308105\n","epoch6, iter50, loss: 0.3503163456916809\n","epoch6, iter60, loss: 0.4453476071357727\n","epoch6, iter70, loss: 0.43455177545547485\n","epoch6, iter80, loss: 0.3945884108543396\n","epoch6, iter90, loss: 0.3663308024406433\n","Finish epoch 6, time elapsed 75.84235620498657\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch6, MAE: 0.0075498963706195354, meanIoU: 0.764193075229296, f_measure: 0.7156378198416687, IoUs: [0.99215661 0.53622954]\n","epoch7, iter0, loss: 0.41825443506240845\n","epoch7, iter10, loss: 0.36396297812461853\n","epoch7, iter20, loss: 0.37703031301498413\n","epoch7, iter30, loss: 0.40906450152397156\n","epoch7, iter40, loss: 0.26135528087615967\n","epoch7, iter50, loss: 0.3875126242637634\n","epoch7, iter60, loss: 0.29446959495544434\n","epoch7, iter70, loss: 0.3720223307609558\n","epoch7, iter80, loss: 0.25397032499313354\n","epoch7, iter90, loss: 0.3876006603240967\n","Finish epoch 7, time elapsed 76.10450339317322\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch7, MAE: 0.0063954927027225494, meanIoU: 0.7490124551818529, f_measure: 0.7671766265598077, IoUs: [0.99337963 0.50464528]\n","epoch8, iter0, loss: 0.27763280272483826\n","epoch8, iter10, loss: 0.32262736558914185\n","epoch8, iter20, loss: 0.3356722593307495\n","epoch8, iter30, loss: 0.45499372482299805\n","epoch8, iter40, loss: 0.36221742630004883\n","epoch8, iter50, loss: 0.30536139011383057\n","epoch8, iter60, loss: 0.254625529050827\n","epoch8, iter70, loss: 0.3694150447845459\n","epoch8, iter80, loss: 0.30902349948883057\n","epoch8, iter90, loss: 0.3073052763938904\n","Finish epoch 8, time elapsed 75.85764718055725\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch8, MAE: 0.00887454953044653, meanIoU: 0.7490019991724308, f_measure: 0.6883452414003662, IoUs: [0.99082927 0.50717473]\n","epoch9, iter0, loss: 0.250238835811615\n","epoch9, iter10, loss: 0.3106485605239868\n","epoch9, iter20, loss: 0.2493896782398224\n","epoch9, iter30, loss: 0.1422516256570816\n","epoch9, iter40, loss: 0.3213915228843689\n","epoch9, iter50, loss: 0.18343399465084076\n","epoch9, iter60, loss: 0.34414029121398926\n","epoch9, iter70, loss: 0.23261670768260956\n","epoch9, iter80, loss: 0.31912317872047424\n","epoch9, iter90, loss: 0.21142223477363586\n","Finish epoch 9, time elapsed 75.80465745925903\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: RuntimeWarning: invalid value encountered in long_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["epoch9, MAE: 0.006306839175522327, meanIoU: 0.7393938476018713, f_measure: 0.7620509812519386, IoUs: [0.99347052 0.48531718]\n","epoch10, iter0, loss: 0.12767243385314941\n","epoch10, iter10, loss: 0.21830570697784424\n","epoch10, iter20, loss: 0.2732982635498047\n","epoch10, iter30, loss: 0.18280725181102753\n","epoch10, iter40, loss: 0.20096644759178162\n","epoch10, iter50, loss: 0.19746112823486328\n","epoch10, iter60, loss: 0.2626603841781616\n","epoch10, iter70, loss: 0.25997287034988403\n","epoch10, iter80, loss: 0.25703567266464233\n","epoch10, iter90, loss: 0.25685903429985046\n","Finish epoch 10, time elapsed 75.82581543922424\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch10, MAE: 0.0056180190294981, meanIoU: 0.7986887521406427, f_measure: 0.7660097745378338, IoUs: [0.9941768 0.6032007]\n","epoch11, iter0, loss: 0.2657853364944458\n","epoch11, iter10, loss: 0.2182619869709015\n","epoch11, iter20, loss: 0.2273305356502533\n","epoch11, iter30, loss: 0.1941700577735901\n","epoch11, iter40, loss: 0.2362668216228485\n","epoch11, iter50, loss: 0.18204520642757416\n","epoch11, iter60, loss: 0.18374072015285492\n","epoch11, iter70, loss: 0.217499777674675\n","epoch11, iter80, loss: 0.1802358478307724\n","epoch11, iter90, loss: 0.16149184107780457\n","Finish epoch 11, time elapsed 75.99434876441956\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch11, MAE: 0.005397300701588392, meanIoU: 0.796417265797503, f_measure: 0.7799714840282498, IoUs: [0.9944032  0.59843133]\n","epoch12, iter0, loss: 0.20084097981452942\n","epoch12, iter10, loss: 0.2152067869901657\n","epoch12, iter20, loss: 0.14225298166275024\n","epoch12, iter30, loss: 0.21708987653255463\n","epoch12, iter40, loss: 0.15604856610298157\n","epoch12, iter50, loss: 0.16200454533100128\n","epoch12, iter60, loss: 0.18226730823516846\n","epoch12, iter70, loss: 0.2124451845884323\n","epoch12, iter80, loss: 0.21938014030456543\n","epoch12, iter90, loss: 0.18195092678070068\n","Finish epoch 12, time elapsed 75.37691354751587\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch12, MAE: 0.005146675277501345, meanIoU: 0.804892102051779, f_measure: 0.7970244128713464, IoUs: [0.99465716 0.61512705]\n","epoch13, iter0, loss: 0.2172483652830124\n","epoch13, iter10, loss: 0.15268763899803162\n","epoch13, iter20, loss: 0.13704094290733337\n","epoch13, iter30, loss: 0.14380058646202087\n","epoch13, iter40, loss: 0.15541920065879822\n","epoch13, iter50, loss: 0.22363846004009247\n","epoch13, iter60, loss: 0.1671469658613205\n","epoch13, iter70, loss: 0.17942623794078827\n","epoch13, iter80, loss: 0.16630548238754272\n","epoch13, iter90, loss: 0.22533568739891052\n","Finish epoch 13, time elapsed 75.83630275726318\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch13, MAE: 0.00509578688070178, meanIoU: 0.8029870847279581, f_measure: 0.8047445913316175, IoUs: [0.99471594 0.61125823]\n","epoch14, iter0, loss: 0.17596885561943054\n","epoch14, iter10, loss: 0.11198294162750244\n","epoch14, iter20, loss: 0.17849205434322357\n","epoch14, iter30, loss: 0.16240426898002625\n","epoch14, iter40, loss: 0.07917740941047668\n","epoch14, iter50, loss: 0.12200560420751572\n","epoch14, iter60, loss: 0.15327392518520355\n","epoch14, iter70, loss: 0.18979156017303467\n","epoch14, iter80, loss: 0.14080926775932312\n","epoch14, iter90, loss: 0.12539280951023102\n","Finish epoch 14, time elapsed 75.78654718399048\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch14, MAE: 0.005549735855311155, meanIoU: 0.7876019011970619, f_measure: 0.7657396647463036, IoUs: [0.9942422 0.5809616]\n","epoch15, iter0, loss: 0.17143160104751587\n","epoch15, iter10, loss: 0.17482058703899384\n","epoch15, iter20, loss: 0.052481770515441895\n","epoch15, iter30, loss: 0.1848718523979187\n","epoch15, iter40, loss: 0.12280653417110443\n","epoch15, iter50, loss: 0.13922709226608276\n","epoch15, iter60, loss: 0.15544167160987854\n","epoch15, iter70, loss: 0.10726438462734222\n","epoch15, iter80, loss: 0.16421093046665192\n","epoch15, iter90, loss: 0.07169625163078308\n","Finish epoch 15, time elapsed 75.83047819137573\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch15, MAE: 0.005089912563562393, meanIoU: 0.8025494159560225, f_measure: 0.8053667090598452, IoUs: [0.99472273 0.6103761 ]\n","epoch16, iter0, loss: 0.14974652230739594\n","epoch16, iter10, loss: 0.1565127968788147\n","epoch16, iter20, loss: 0.11440696567296982\n","epoch16, iter30, loss: 0.08613377809524536\n","epoch16, iter40, loss: 0.15785685181617737\n","epoch16, iter50, loss: 0.12932783365249634\n","epoch16, iter60, loss: 0.10924842208623886\n","epoch16, iter70, loss: 0.131182461977005\n","epoch16, iter80, loss: 0.09966817498207092\n","epoch16, iter90, loss: 0.08065696805715561\n","Finish epoch 16, time elapsed 75.62389588356018\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch16, MAE: 0.0050559998489916325, meanIoU: 0.8057632213152515, f_measure: 0.8142788494950953, IoUs: [0.99475305 0.61677339]\n","epoch17, iter0, loss: 0.12898167967796326\n","epoch17, iter10, loss: 0.10225531458854675\n","epoch17, iter20, loss: 0.067634716629982\n","epoch17, iter30, loss: 0.12922067940235138\n","epoch17, iter40, loss: 0.1120939701795578\n","epoch17, iter50, loss: 0.09997345507144928\n","epoch17, iter60, loss: 0.11812609434127808\n","epoch17, iter70, loss: 0.1306934356689453\n","epoch17, iter80, loss: 0.11182153224945068\n","epoch17, iter90, loss: 0.048436325043439865\n","Finish epoch 17, time elapsed 75.72042441368103\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch17, MAE: 0.004873924423009157, meanIoU: 0.8135395471256793, f_measure: 0.8169410680353131, IoUs: [0.99494514 0.63213395]\n","epoch18, iter0, loss: 0.11791684478521347\n","epoch18, iter10, loss: 0.07369808852672577\n","epoch18, iter20, loss: 0.14576002955436707\n","epoch18, iter30, loss: 0.0743609219789505\n","epoch18, iter40, loss: 0.09538088738918304\n","epoch18, iter50, loss: 0.06761252135038376\n","epoch18, iter60, loss: 0.08133959770202637\n","epoch18, iter70, loss: 0.12292321771383286\n","epoch18, iter80, loss: 0.0748247504234314\n","epoch18, iter90, loss: 0.07730495184659958\n","Finish epoch 18, time elapsed 75.95407032966614\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch18, MAE: 0.005174522288143635, meanIoU: 0.788216491236418, f_measure: 0.8317889685477191, IoUs: [0.99464495 0.58178803]\n","epoch19, iter0, loss: 0.11514964699745178\n","epoch19, iter10, loss: 0.09458450973033905\n","epoch19, iter20, loss: 0.08627836406230927\n","epoch19, iter30, loss: 0.051038920879364014\n","epoch19, iter40, loss: 0.0893775224685669\n","epoch19, iter50, loss: 0.10026536881923676\n","epoch19, iter60, loss: 0.056665316224098206\n","epoch19, iter70, loss: 0.07308220863342285\n","epoch19, iter80, loss: 0.06557179242372513\n","epoch19, iter90, loss: 0.10639812797307968\n","Finish epoch 19, time elapsed 75.9074194431305\n","---------iter=0\n","/content/CamouflageData/gt/dataset16_09_00013023.png\n","['/content/CamouflageData/gt/dataset16_09_00013023.png']\n","---------iter=1\n","/content/CamouflageData/gt/dataset15_03_00003495.png\n","['/content/CamouflageData/gt/dataset15_03_00003495.png']\n","---------iter=2\n","/content/CamouflageData/gt/dataset16_02_00002904.png\n","['/content/CamouflageData/gt/dataset16_02_00002904.png']\n","---------iter=3\n","/content/CamouflageData/gt/dataset11_12_00016167.png\n","['/content/CamouflageData/gt/dataset11_12_00016167.png']\n","---------iter=4\n","/content/CamouflageData/gt/dataset08_07_00007923.png\n","['/content/CamouflageData/gt/dataset08_07_00007923.png']\n","epoch19, MAE: 0.005734825041145086, meanIoU: 0.7552001801942658, f_measure: 0.7796356169536403, IoUs: [0.99407559 0.51632478]\n","The highest mIOU is 0.8135395471256793 and is achieved at epoch-17\n","The lowest MAE is 0.004873924423009157 and is achieved at epoch-17\n","The highest f-measure is 0.8317889685477191 and is achieved at epoch-18\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qjgxD-ulq43O","colab_type":"code","outputId":"8fa48643-e62d-46bb-a189-0cebc255a3aa","executionInfo":{"status":"ok","timestamp":1578495631642,"user_tz":-480,"elapsed":2346,"user":{"displayName":"史孟玄","photoUrl":"","userId":"05623711265995213758"}},"colab":{"base_uri":"https://localhost:8080/","height":353}},"source":["!zip file /content/CamouflageData/result_comparision/*"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  adding: content/CamouflageData/result_comparision/0.jpg (deflated 9%)\n","  adding: content/CamouflageData/result_comparision/10.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/11.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/12.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/13.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/14.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/15.jpg (deflated 12%)\n","  adding: content/CamouflageData/result_comparision/16.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/17.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/18.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/19.jpg (deflated 12%)\n","  adding: content/CamouflageData/result_comparision/1.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/2.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/3.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/4.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/5.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/6.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/7.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/8.jpg (deflated 11%)\n","  adding: content/CamouflageData/result_comparision/9.jpg (deflated 12%)\n"],"name":"stdout"}]}]}